---
title: "Mini Project 3: Model Diagnostics for Teenage Gambling Data"
author: "Your Name"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```

## Introduction

This mini-project explores a linear regression model fitted to the `teengamb` dataset from the **faraway** R package. The goal is to diagnose potential violations of model assumptions, such as constant variance, normality of residuals, as well as to detect leverage points, outliers, and influential observations.

<br>

## Data and Model

We begin by loading the necessary packages and fitting our linear model (`lmod`) predicting annual gambling expenditure (`gamble`) as a function of sex, socioeconomic status (`status`), weekly income, and verbal score (`verbal`).

```{r libraries-and-data}
# Load required libraries
library(faraway)
library(car) # For avPlots, crPlots

# Fit the linear model
lmod <- lm(gamble ~ ., data = teengamb)
summary(lmod)
```

**Interpretation of model summary**  
- **sex** appears significant (with negative coefficient, meaning females spend less than males on gambling).
- **income** is the most significant predictor, strongly associated with gambling expenditure.
- **status** and **verbal** appear not to be statistically significant in this model.

<br>

## Checking the Constant Variance Assumption

A fundamental assumption of linear regression is that the variance of the residuals (errors) is constant (homoscedasticity). We first use a residuals-versus-fitted-values plot:

```{r rvf-plot}
plot(lmod, which = 1)  # Residual vs Fitted plot
```

### Observations
- There is a fanning out in the residuals, suggesting the variance of the residuals increases for larger fitted values (possible heteroscedasticity).

We then split residuals into two groups based on the median value of `teengamb$gamble` and use `var.test` to compare their variances:

```{r check-variance-median}
residuals <- resid(lmod)
med_gamble <- median(teengamb$gamble)

group1 <- residuals[teengamb$gamble <= med_gamble]
group2 <- residuals[teengamb$gamble > med_gamble]

var.test(group1, group2)
```

A small p-value here indicates we reject the null hypothesis of equal variances.

#### Residuals vs. Income

Because **income** is the most significant predictor, we also examine a residuals-versus-income plot:

```{r rv-income}
plot(teengamb$income, residuals,
     xlab = "Income", ylab = "Residuals")
abline(h = 0, col = 'red')
```

We similarly split fitted values at the median of `income` and perform another F-test:

```{r var-test-income}
fitted_values <- predict(lmod)
lower_group <- residuals[fitted_values < median(teengamb$income)]
higher_group <- residuals[fitted_values >= median(teengamb$income)]

var.test(lower_group, higher_group)
```

Again, a small p-value suggests a violation of constant variance.

<br>

## Checking the Normality Assumption

Next, we want the distribution of errors to be approximately normal. We check this visually with a histogram and Q-Q plot:

```{r normality-plots}
hist(residuals, breaks = 20, 
     main = "Histogram of Residuals", 
     xlab = "Residuals")

qqnorm(residuals)
qqline(residuals)
```

### Observations
- The histogram appears roughly bell-shaped but shows some heavy tails.
- The Q-Q plot confirms heavier-than-expected tails, with some extreme positive residuals and a few extreme negatives.

We use two formal normality tests:

```{r normality-tests}
shapiro.test(residuals)
ks.test(residuals, "pnorm")
```

Both tests yield small p-values, rejecting the null of normality.

<br>

## Identifying High Leverage Points

High leverage points are observations that lie far from the “center” of the predictors. We examine the diagonal of the hat matrix:

```{r leverage}
# Calculate leverage values
leverage_values <- lm.influence(lmod)$hat

# Create a data frame
lev_df <- data.frame(obs = 1:nrow(teengamb),
                     leverage = leverage_values)
lev_df <- lev_df[order(-lev_df$leverage), ]

# Threshold for high leverage
mean_lev <- mean(leverage_values)
high_leverage_df <- lev_df[lev_df$leverage > 2 * mean_lev, ]
high_leverage_df
```

We see four high leverage points that exceed twice the mean leverage. For a quick visualization, a half-normal plot:

```{r halfnorm-leverage}
obs <- 1:nrow(teengamb)
halfnorm(leverage_values, labs = obs, 
         ylab = "Leverages")
qqline(leverage_values, distribution = qnorm, 
       col = "blue", lty = 2)
```

<br>

## Identifying Outliers with Studentized Residuals

Outliers are points where the observed response is far from the model’s predicted value. We examine the studentized residuals (also called externally studentized). Then, we apply a Bonferroni correction for multiple testing:

```{r outliers-studentized}
stud_res <- rstudent(lmod)
abs_stud <- abs(stud_res)

# Sort by magnitude
stud_df <- data.frame(obs = 1:length(stud_res),
                      studentized = stud_res,
                      abs_stud = abs_stud)
stud_df <- stud_df[order(-stud_df$abs_stud), ]
head(stud_df)

# Bonferroni-corrected threshold
n <- nrow(teengamb)
df <- lmod$df.residual
alpha <- 0.05
bonf_alpha <- alpha / n

critical_value <- qt(1 - bonf_alpha/2, df)
critical_value
```

Any studentized residual that exceeds this critical value in absolute magnitude is considered an outlier. We find one data point (obs #24) that substantially exceeds this threshold.

#### Plotting Actual vs. Predicted to Highlight Outlier

```{r actual-vs-pred}
actual <- teengamb$gamble
predicted <- predict(lmod)

high_resid_pts <- abs_stud > critical_value

plot(actual, predicted,
     xlab = "Actual Values", ylab = "Predicted Values",
     main = "Predicted vs. Actual",
     pch = 19,
     col = ifelse(high_resid_pts, "red", "black"))
abline(a = 0, b = 1, col = "blue", lty = 2)
legend("topleft", legend = c("High Residual", "Normal"),
       col = c("red", "black"), pch = 19)
```

<br>

## Influential Observations

While outliers are about large residuals, “influence” focuses on how much a point changes the regression when removed. A common measure is Cook’s Distance:

```{r cooks-distance}
cook <- cooks.distance(lmod)
threshold_cook <- 4 / n  # Common rule-of-thumb cutoff

halfnorm(cook, labs = obs, ylab = "Cook's Distance")
abline(h = threshold_cook, col = "red", lty = 2)

high_cook_obs <- which(cook > threshold_cook)
high_cook_obs
```

We see two points that exceed this cutoff. Point #24 matches our outlier from before, and point #39 shows moderate Cook’s Distance despite not having a massive studentized residual or extremely high leverage.

### Effect of Removing Influential Points

We can refit the model omitting each influential point and compare parameter estimates:

```{r remove-24}
# Remove observation 24
teengamb_no24 <- teengamb[-24, ]
lmod_no24 <- lm(gamble ~ ., data = teengamb_no24)
summary(lmod_no24)
```

Compare with the original model:

```{r compare-24}
coef_diff <- coef(lmod) - coef(lmod_no24)
r2_diff   <- summary(lmod)$r.squared - summary(lmod_no24)$r.squared
sigma_diff <- summary(lmod)$sigma - summary(lmod_no24)$sigma

coef_diff
r2_diff
sigma_diff
```

Observation #24 strongly influences the intercept, sex coefficient, and the model’s residual standard error.

```{r remove-39}
# Remove observation 39
teengamb_no39 <- teengamb[-39, ]
lmod_no39 <- lm(gamble ~ ., data = teengamb_no39)
summary(lmod_no39)

# Compare with original
coef_diff_39 <- coef(lmod) - coef(lmod_no39)
r2_diff_39   <- summary(lmod)$r.squared - summary(lmod_no39)$r.squared
sigma_diff_39 <- summary(lmod)$sigma - summary(lmod_no39)$sigma

coef_diff_39
r2_diff_39
sigma_diff_39
```

The changes are smaller than for observation #24, but still non-trivial (e.g., the income coefficient changes by about 0.57).

<br>

## Checking Model Structure

Added-variable plots (partial regression plots) and component+residual plots help visualize whether each predictor has a linear relationship with the response and whether any observations stand out.

### Added-Variable Plots

```{r av-plots}
avPlots(lmod, terms = ~ income, ask = FALSE)
avPlots(lmod, terms = ~ sex,    ask = FALSE)
avPlots(lmod, terms = ~ status, ask = FALSE)
avPlots(lmod, terms = ~ verbal, ask = FALSE)
```

We see points #24 and #39 are distinct in many plots, consistent with earlier findings.

### Component + Residual Plots

```{r cr-plots}
suppressWarnings(crPlots(lmod, terms = ~ income, ask=FALSE))
suppressWarnings(crPlots(lmod, terms = ~ sex, ask=FALSE))
suppressWarnings(crPlots(lmod, terms = ~ status, ask=FALSE))
suppressWarnings(crPlots(lmod, terms = ~ verbal, ask=FALSE))
```

These partial residual plots confirm no strong sign of nonlinearity, with the relationship to **income** being the clearest.

<br>

## Conclusion

From our diagnostic analysis:

- **Constant Variance**: Tests and plots suggest clear heteroscedasticity, especially at higher fitted values and higher incomes.
- **Normality**: Residuals show heavier tails than a normal distribution. Formal tests (Shapiro-Wilk and Kolmogorov-Smirnov) reject normality.
- **High Leverage**: Four points exceed twice the mean leverage. They largely differ in `income` (and in some cases `status` or `verbal`).
- **Outliers**: One particularly large outlier (#24) strongly deviates from the fitted model prediction (studentized residual ~6).
- **Influential Points**: Observations #24 and #39 have high Cook’s Distance. Removing #24 notably changes the model intercept, slope estimates, and residual standard error. Removing #39 has a more moderate effect.

In summary, the model’s assumptions are not fully met. The presence of heteroscedasticity and non-normality of residuals suggests we may need transformations or more robust modeling techniques. Observations #24 and #39 warrant special attention due to their influence on the model.

<br>

---

**Reference**:  
Ide-Smith & Lea (1988). *Journal of Gambling Behavior*, 4, 110–118.

<br>

*End of Document*